AWSTemplateFormatVersion: '2010-09-09'
Description: 'Kubernetes Cluster'

Parameters:
  ClusterName:
    Description: Easily-identifiable name for your cluster. Defaults to 'k8s'.
    Type: String
    Default: 'k8s'
  ControlPlaneAmiId:
    Description: AMI ID of the Control Plane node(s)
    Type: String
  InstanceType:
    Description: EC2 instance type for all nodes
    Type: String
    Default: t3a.small
  KubernetesDistro:
    Description: Shorthand name for the chosen Kubernetes distribution
    Type: String
    AllowedValues:
      - k3s
      - kubeadm
  PodNetworkCidr:
    Description: |
      Network CIDR for the Cluster's Podsm if applicable. Will always be passed
      in as an env var for control plane user data, but may or may not be used
      depending on your init script configuration for that AMI. This CIDR range
      MUST NOT overlap with your VPC/Subnet CIDR ranges -- e.g. if your VPC CIDR
      is '10.0.0.0/16', and a Subnet is '10.0.2.0/24', your PodNetworkCidr
      should be something like '10.<not-0>.0.0/16'. Some CNIs recommend certain
      ranges specifically. This parameter defaults to '10.244.0.0/16', which is
      what the Flannel CNI recommends.
    Type: String
    Default: '10.244.0.0/16'
  WorkerAmiId:
    Description: AMI ID of the Worker node(s)
    Type: String
  WorkerNodeCount:
    Description: How many worker nodes to deploy to the cluster. Defaults to 1.
    Type: Number
    Default: 1


Resources:

  #################
  # EC2 Resources #
  #################
  ControlPlane:
    Type: AWS::EC2::Instance
    Properties: 
      ImageId: !Ref ControlPlaneAmiId
      InstanceType: !Ref InstanceType
      IamInstanceProfile: !Ref KubernetesInstanceProfile
      SecurityGroupIds:
        - !Ref ControlPlaneSG
      SubnetId: { Fn::ImportValue: !Sub '${ClusterName}-vpc-PrivateSubnet1Id' }
      UserData:
        Fn::Base64: !Sub |
          #!/usr/bin/env bash
          export cluster_name="${ClusterName}"
          export k8s_distro="${KubernetesDistro}"
          export pod_network_cidr="${PodNetworkCidr}"
          bash /root/scripts/init-control-plane-aws.sh
      Tags:
        - Key:   'Name'
          Value: !Sub '${ClusterName}-control-plane'
  
  WorkerLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateName: kubernetes-worker-lt
      LaunchTemplateData:
        ImageId: !Ref WorkerAmiId
        InstanceType: !Ref InstanceType
        IamInstanceProfile:
          Name: !Ref KubernetesInstanceProfile
        SecurityGroupIds:
          - !Ref WorkerSG
        UserData:
          Fn::Base64:
            !Sub |
              #!/usr/bin/env bash
              export cluster_name="${ClusterName}"
              export k8s_distro="${KubernetesDistro}"
              bash /root/scripts/init-worker-aws.sh
        TagSpecifications:
          - ResourceType: instance
            Tags:
              - Key:   'Name'
                Value: !Sub '${ClusterName}-worker'

  WorkerASG:
    Type: AWS::AutoScaling::AutoScalingGroup
    DependsOn: ControlPlane
    Properties:
      AutoScalingGroupName: kubernetes-worker-asg
      LaunchTemplate:
        LaunchTemplateId: !Ref WorkerLaunchTemplate
        Version: !GetAtt WorkerLaunchTemplate.LatestVersionNumber
      Cooldown: 10
      # Currently all pegged as the same value, for now
      DesiredCapacity: !Ref WorkerNodeCount
      MinSize: !Ref WorkerNodeCount
      MaxSize: !Ref WorkerNodeCount
      VPCZoneIdentifier:
        - { Fn::ImportValue: !Sub '${ClusterName}-vpc-PrivateSubnet1Id' }
        - { Fn::ImportValue: !Sub '${ClusterName}-vpc-PrivateSubnet2Id' }

  ControlPlaneSG:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: 'SG for Kubernetes Control Plane nodes'
      VpcId: { Fn::ImportValue: !Sub '${ClusterName}-vpc-VpcId' }
      SecurityGroupIngress:
        - Description: ICMP ping
          IpProtocol: icmp
          FromPort: -1
          ToPort: -1
          CidrIp: { Fn::ImportValue: !Sub '${ClusterName}-vpc-VpcCidr' }
        - Description: etcd
          IpProtocol: tcp
          FromPort: 2379
          ToPort: 2380
          CidrIp: { Fn::ImportValue: !Sub '${ClusterName}-vpc-VpcCidr' }
        - Description: kube-apiserver
          IpProtocol: tcp
          FromPort: 6443
          ToPort: 6443
          CidrIp: { Fn::ImportValue: !Sub '${ClusterName}-vpc-VpcCidr' }
        - Description: Flannel VXLAN
          IpProtocol: udp
          FromPort: 8472
          ToPort: 8472
          CidrIp: { Fn::ImportValue: !Sub '${ClusterName}-vpc-VpcCidr' }
        - Description: Kubelet metrics
          IpProtocol: tcp
          FromPort: 10250
          ToPort: 10252
          CidrIp: { Fn::ImportValue: !Sub '${ClusterName}-vpc-VpcCidr' }
        
  WorkerSG:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: 'SG for Kubernetes Worker nodes'
      VpcId: { Fn::ImportValue: !Sub '${ClusterName}-vpc-VpcId' }
      SecurityGroupIngress:
        - IpProtocol: icmp
          FromPort: -1
          ToPort: -1
          CidrIp: { Fn::ImportValue: !Sub '${ClusterName}-vpc-VpcCidr' }
        - Description: kube-apiserver
          IpProtocol: tcp
          FromPort: 6443
          ToPort: 6443
          CidrIp: { Fn::ImportValue: !Sub '${ClusterName}-vpc-VpcCidr' }
        - Description: Flannel VXLAN
          IpProtocol: udp
          FromPort: 8472
          ToPort: 8472
          CidrIp: { Fn::ImportValue: !Sub '${ClusterName}-vpc-VpcCidr' }
        - Description: Kubelet metrics
          IpProtocol: tcp
          FromPort: 10250
          ToPort: 10250
          CidrIp: { Fn::ImportValue: !Sub '${ClusterName}-vpc-VpcCidr' }
        - Description: Node ports
          IpProtocol: tcp
          FromPort: 30000
          ToPort: 32767
          CidrIp: { Fn::ImportValue: !Sub '${ClusterName}-vpc-VpcCidr' }


  #######
  # IAM #
  #######
  KubernetesRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              Service:
                - 'ec2.amazonaws.com'
            Action:
              - 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore'
        - 'arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess'
      Policies:
        - PolicyName: !Sub 'KubernetesPolicy-${ClusterName}'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 's3:Head*'
                  - 's3:List*'
                Resource: '*'
              - Effect: 'Allow'
                Action:
                  - 's3:GetObject'
                  - 's3:PutObject'
                Resource: !Sub 'arn:aws:s3:::${ClusterName}-${AWS::AccountId}/*'
              - Effect: 'Allow'
                Action:
                  - 'logs:*'
                Resource: !Sub '${LogGroup.Arn}'
              - Effect: 'Allow'
                Action:
                  - 'logs:*'
                Resource: !Sub '${LogGroup.Arn}/*'
  
  KubernetesInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref KubernetesRole


  ###########
  # Logging #
  ###########
  LogGroup:
    Type: AWS::Logs::LogGroup
    Properties: 
      LogGroupName: !Sub '/aws/ec2/kubernetes/${ClusterName}'
      RetentionInDays: 7
  LogStreamControlPlane:
    Type: AWS::Logs::LogStream
    Properties:
      LogGroupName: !Ref LogGroup
      LogStreamName: 'control-plane'
  LogStreamWorkers:
    Type: AWS::Logs::LogStream
    Properties:
      LogGroupName: !Ref LogGroup
      LogStreamName: 'workers'
